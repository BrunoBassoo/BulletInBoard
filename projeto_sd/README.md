# BulletInBoard - Sistema de Mensageria DistribuÃ­da

Projeto de Sistemas DistribuÃ­dos - Sistema de mensageria com consistÃªncia e replicaÃ§Ã£o.

## ğŸ“‹ VisÃ£o Geral

Sistema de mensageria distribuÃ­da implementado com:
- **ZeroMQ** para comunicaÃ§Ã£o
- **MessagePack** para serializaÃ§Ã£o
- **Docker** para orquestraÃ§Ã£o
- **ReplicaÃ§Ã£o** para consistÃªncia de dados

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ReferÃªncia  â”‚ (5560) - Gerencia ranks e heartbeats
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚        â”‚         â”‚
â”Œâ”€â”€â–¼â”€â”€â”  â”Œâ”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”
â”‚ S1  â”‚  â”‚ S2  â”‚  â”‚ S3  â”‚ (3 rÃ©plicas)
â”‚5561 â”‚  â”‚5561 â”‚  â”‚5561 â”‚ - SincronizaÃ§Ã£o
â”‚5562 â”‚  â”‚5562 â”‚  â”‚5562 â”‚ - ReplicaÃ§Ã£o
â””â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜
   â”‚        â”‚        â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚       â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”‚
    â”‚Broker â”‚   â”‚ (5555/5556)
    â”‚ROUTER â”‚   â”‚
    â”‚DEALER â”‚   â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â”‚
        â”‚       â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”‚
    â”‚Clienteâ”‚   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                â”‚
            â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
            â”‚ Proxy  â”‚ (5557/5558)
            â”‚ XSUB   â”‚
            â”‚ XPUB   â”‚
            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚Publisherâ”‚    â”‚ Subscriber  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Parte 5: ConsistÃªncia e ReplicaÃ§Ã£o

### Problema

O broker usa **round-robin** para balancear carga entre os servidores. Isso significa:
- Cada servidor recebe apenas **1/3 das mensagens** (com 3 rÃ©plicas)
- Se um servidor falhar, **perde-se parte do histÃ³rico**
- Clientes que consultam servidores diferentes veem **dados diferentes**

**Exemplo:**
```
Cliente 1 -> Broker -> Servidor 1 (login: "alice")
Cliente 2 -> Broker -> Servidor 2 (login: "bob")
Cliente 3 -> Broker -> Servidor 3 (login: "charlie")

Servidor 1 sÃ³ conhece: alice
Servidor 2 sÃ³ conhece: bob
Servidor 3 sÃ³ conhece: charlie
```

### SoluÃ§Ã£o Implementada

#### MÃ©todo Escolhido: **ReplicaÃ§Ã£o Passiva com PropagaÃ§Ã£o AssÃ­ncrona**

##### Por que este mÃ©todo?

1. **Simplicidade**: FÃ¡cil de implementar e entender
2. **Performance**: NÃ£o bloqueia o servidor principal
3. **Disponibilidade**: Sistema continua funcionando mesmo se alguns servidores falharem
4. **ConsistÃªncia Eventual**: Dados convergem com o tempo

##### CaracterÃ­sticas do MÃ©todo

- **Master-Master**: Qualquer servidor pode receber escritas
- **PropagaÃ§Ã£o AssÃ­ncrona**: ReplicaÃ§Ã£o nÃ£o bloqueia resposta ao cliente
- **IdempotÃªncia**: Dados duplicados sÃ£o ignorados
- **DetecÃ§Ã£o de Loops**: Marcador `"replicated": true` evita replicaÃ§Ã£o infinita

### Como Funciona

#### 1. Cliente Envia Dados

```
Cliente -> Broker -> Servidor 1 (round-robin)
```

#### 2. Servidor Processa e Responde

```python
# Servidor 1 processa
usuarios.append({"user": "alice", "timestamp": ...})
salvar_usuarios(usuarios)  # Salva em JSON

# Responde ao cliente
reply = {"service": "login", "data": {"status": "sucesso"}}
socket.send(reply)
```

#### 3. Servidor Replica para Outros (AssÃ­ncrono)

```python
# Em thread separada (nÃ£o bloqueia)
replicar_para_outros_servidores({
    "service": "login",
    "data": {"user": "alice", "timestamp": ...},
    "replicated": True  # Marcador importante!
})
```

#### 4. Outros Servidores Recebem e Salvam

```python
# Servidor 2 e 3 recebem via porta 5562
if request.get("replicated"):
    # NÃ£o replicar novamente (evita loop!)
    if not usuario_existe:
        usuarios.append(usuario)
        salvar_usuarios(usuarios)
    # Enviar ACK
```

#### 5. Resultado Final

```
Servidor 1: [alice, bob, charlie]
Servidor 2: [alice, bob, charlie]
Servidor 3: [alice, bob, charlie]
```

âœ… **Todos os servidores tÃªm todos os dados!**

### Diagrama de SequÃªncia

```
Cliente     Broker      S1          S2          S3
  |           |          |           |           |
  |--login--->|          |           |           |
  |           |--RR----->|           |           |
  |           |          |--salva----|           |
  |           |<--OK-----|           |           |
  |<--OK------|          |           |           |
  |           |          |--replica->|           |
  |           |          |--replica----------->  |
  |           |          |           |--salva----|
  |           |          |           |           |--salva--
  |           |          |<---ACK----|           |
  |           |          |<---ACK---------------|
```

### CaracterÃ­sticas da ImplementaÃ§Ã£o

#### Porta de ReplicaÃ§Ã£o: 5562

Cada servidor abre uma porta `5562` para receber dados replicados de outros servidores.

#### Marcador de ReplicaÃ§Ã£o

```python
mensagem["replicated"] = True
```

- Previne **loop infinito** de replicaÃ§Ã£o
- Dados replicados **nÃ£o sÃ£o replicados novamente**

#### Busca DinÃ¢mica de Servidores

```python
lista_servidores = obter_lista_servidores()
for servidor in lista_servidores:
    if servidor["name"] != NOME_SERVIDOR:
        # Replicar para este servidor
```

- Usa o **Servidor de ReferÃªncia** para descobrir servidores ativos
- Adapta-se automaticamente ao nÃºmero de rÃ©plicas

#### Volume Compartilhado

```yaml
volumes:
  dados_compartilhados:/app/dados
```

- Todos os servidores salvam em `/app/dados/`
- Arquivos JSON compartilhados:
  - `usuarios.json`
  - `canais.json`
  - `publicacoes.json`
  - `mensagens.json`

#### Processamento de ReplicaÃ§Ã£o

```python
if replication_socket in socks:
    request = recv()
    if request.get("replicated"):
        # Processar sem replicar novamente
        salvar_dados()
        enviar_ack()
```

### Dados Replicados

Todos os tipos de dados sÃ£o replicados:

1. âœ… **Login de usuÃ¡rios** (`service: "login"`)
2. âœ… **CriaÃ§Ã£o de canais** (`service: "channel"`)
3. âœ… **PublicaÃ§Ãµes em canais** (`service: "publish"`)
4. âœ… **Mensagens privadas** (`service: "message"`)

### Garantias de ConsistÃªncia

#### ConsistÃªncia Eventual

- Dados **convergem** com o tempo
- Todos os servidores eventualmente terÃ£o os mesmos dados
- LatÃªncia tÃ­pica: **< 1 segundo**

#### IdempotÃªncia

- UsuÃ¡rio/canal jÃ¡ existente: **nÃ£o duplica**
- VerificaÃ§Ã£o antes de adicionar:
  ```python
  if not any(u.get("user") == user for u in usuarios):
      usuarios.append(usuario)
  ```

#### OrdenaÃ§Ã£o Causal (RelÃ³gio LÃ³gico)

- Todas as mensagens incluem `clock`
- Eventos mantÃªm ordem causal
- Conflitos resolvidos por timestamp

### TolerÃ¢ncia a Falhas

#### Servidor Offline

- ReplicaÃ§Ã£o falha **silenciosamente** (sem erro)
- Quando servidor volta, receberÃ¡ novos dados
- Dados antigos: recuperados do volume compartilhado

#### Perda de Mensagem de ReplicaÃ§Ã£o

- Servidor de origem mantÃ©m dados
- PrÃ³xima operaÃ§Ã£o pode trazer consistÃªncia
- Volume compartilhado ajuda na recuperaÃ§Ã£o

#### PartiÃ§Ã£o de Rede

- Cada partiÃ§Ã£o continua operando
- Quando reconectar, dados convergem
- PossÃ­vel inconsistÃªncia temporÃ¡ria

### ModificaÃ§Ãµes em RelaÃ§Ã£o a MÃ©todos ClÃ¡ssicos

#### 1. ReplicaÃ§Ã£o Master-Master (nÃ£o Master-Slave)

**ClÃ¡ssico:**
- Um servidor Ã© master, outros sÃ£o slaves
- Escritas sÃ³ no master

**Nossa implementaÃ§Ã£o:**
- Todos os servidores aceitam escritas
- ReplicaÃ§Ã£o peer-to-peer
- Maior disponibilidade

#### 2. Volume Compartilhado + ReplicaÃ§Ã£o

**ClÃ¡ssico:**
- Apenas replicaÃ§Ã£o via rede OU apenas compartilhamento

**Nossa implementaÃ§Ã£o:**
- **Volume compartilhado**: persistÃªncia comum
- **ReplicaÃ§Ã£o via rede**: atualizaÃ§Ã£o imediata
- **Dupla garantia** de consistÃªncia

#### 3. Descoberta DinÃ¢mica de Servidores

**ClÃ¡ssico:**
- Lista fixa de servidores

**Nossa implementaÃ§Ã£o:**
- Consulta **Servidor de ReferÃªncia**
- Adapta-se ao nÃºmero de rÃ©plicas
- NÃ£o precisa configuraÃ§Ã£o manual

### Formato das Mensagens de ReplicaÃ§Ã£o

```json
{
  "service": "login",
  "data": {
    "user": "alice",
    "timestamp": 1699547123.456,
    "clock": 42
  },
  "replicated": true
}
```

Campo `"replicated": true` Ã© **essencial** para:
- Identificar mensagens replicadas
- Evitar loop infinito
- Processar sem replicar novamente

### Troca de Mensagens Entre Servidores

#### Protocolo de ReplicaÃ§Ã£o

**RequisiÃ§Ã£o (Servidor Origem -> Servidor Destino):**

```
Porta: 5562
Socket: REQ/REP
SerializaÃ§Ã£o: MessagePack

Mensagem: {
  "service": "login",
  "data": {...},
  "replicated": true
}
```

**Resposta (ACK):**

```json
{
  "status": "OK",
  "clock": 43
}
```

#### Fluxo Completo

1. **Cliente faz login no S1:**
   - S1 processa e salva
   - S1 responde ao cliente
   - S1 inicia replicaÃ§Ã£o (thread separada)

2. **S1 obtÃ©m lista de servidores:**
   - Consulta Servidor de ReferÃªncia
   - Recebe: `[{name: "S1", rank: 1}, {name: "S2", rank: 2}, {name: "S3", rank: 3}]`

3. **S1 replica para S2 e S3:**
   - Thread 1: `tcp://S2:5562` <- mensagem + `replicated: true`
   - Thread 2: `tcp://S3:5562` <- mensagem + `replicated: true`

4. **S2 e S3 recebem:**
   - Verificam `replicated == true`
   - Salvam dados (sem replicar novamente!)
   - Enviam ACK

5. **Estado final:**
   - S1, S2, S3: todos tÃªm o login de alice

### Performance

#### LatÃªncia

- **Cliente -> Servidor**: ~10ms (sem replicaÃ§Ã£o)
- **ReplicaÃ§Ã£o**: assÃ­ncrona, nÃ£o afeta cliente
- **ConvergÃªncia**: < 1 segundo (rede local)

#### Throughput

- ReplicaÃ§Ã£o em **threads paralelas**
- NÃ£o bloqueia processamento de clientes
- EscalÃ¡vel para N servidores

### Testes e ValidaÃ§Ã£o

#### Teste 1: Dados Replicados

```bash
# 1. Fazer login no servidor 1
docker attach cliente
> OpÃ§Ã£o 1: bruno

# 2. Ver logs de TODOS os servidores
docker-compose logs servidor

# Deve aparecer em S1, S2 e S3:
# [S] - Login do bruno feito!
# [S] âœ… 1 usuÃ¡rios salvos em usuarios.json
# [S] ReplicaÃ§Ã£o: usuÃ¡rio 'bruno' adicionado (S2 e S3)

# 3. Listar usuÃ¡rios (pode cair em qualquer servidor)
> OpÃ§Ã£o 2

# Resultado: todos os servidores retornam 'bruno'
```

#### Teste 2: Volume Compartilhado

```bash
# 1. Cadastrar dados
docker attach cliente
> OpÃ§Ã£o 1: alice
> OpÃ§Ã£o 3: tecnologia

# 2. Reiniciar servidores
docker-compose restart servidor

# 3. Listar novamente
> OpÃ§Ã£o 2: alice ainda aparece âœ…
> OpÃ§Ã£o 4: tecnologia ainda aparece âœ…
```

#### Teste 3: Falha de Servidor

```bash
# 1. Parar um servidor
docker stop <container_id_servidor1>

# 2. Fazer login (cairÃ¡ em S2 ou S3)
> OpÃ§Ã£o 1: bob

# 3. S2 ou S3 tenta replicar
# - S1: falha (servidor parado)
# - S3 (ou S2): sucesso

# 4. Reiniciar S1
docker start <container_id_servidor1>

# 5. S1 carrega dados do volume compartilhado
# - Tem alice (dados antigos)
# - Tem bob (via volume compartilhado)
```

## ğŸ” Garantias de ConsistÃªncia

### Modelo de ConsistÃªncia: **Eventual**

- âœ… Todos os servidores eventualmente convergem
- âœ… NÃ£o hÃ¡ perda de dados (assumindo que pelo menos 1 servidor estÃ¡ ativo)
- âš ï¸ PossÃ­vel inconsistÃªncia temporÃ¡ria (< 1 segundo)
- âœ… Leituras podem retornar dados ligeiramente desatualizados

### ResoluÃ§Ã£o de Conflitos

#### EstratÃ©gia: **First-Write-Wins**

```python
if not any(u.get("user") == user for u in usuarios):
    usuarios.append(usuario)
```

- Primeira escrita de um dado prevalece
- Duplicatas sÃ£o ignoradas
- Baseado no **timestamp** da operaÃ§Ã£o

### AnÃ¡lise CAP

- **C (Consistency)**: ConsistÃªncia Eventual âš ï¸
- **A (Availability)**: Alta Disponibilidade âœ…
- **P (Partition Tolerance)**: Tolerante a PartiÃ§Ãµes âœ…

**Escolha:** AP (Availability + Partition Tolerance)

## ğŸ“Š Estrutura de Dados Persistidos

### `/app/dados/usuarios.json`

```json
[
  {
    "user": "alice",
    "timestamp": 1699547123.456
  },
  {
    "user": "bob",
    "timestamp": 1699547134.789
  }
]
```

### `/app/dados/canais.json`

```json
[
  {
    "channel": "geral",
    "timestamp": 1699547200.123
  },
  {
    "channel": "tecnologia",
    "timestamp": 1699547210.456
  }
]
```

### `/app/dados/publicacoes.json`

```json
[
  {
    "user": "alice",
    "channel": "geral",
    "message": "OlÃ¡ a todos!",
    "timestamp": 1699547250.123
  }
]
```

### `/app/dados/mensagens.json`

```json
[
  {
    "src": "alice",
    "dst": "bob",
    "message": "Oi Bob!",
    "timestamp": 1699547300.456
  }
]
```

## ğŸ”Œ Portas Utilizadas

| Porta | ServiÃ§o | FunÃ§Ã£o |
|-------|---------|--------|
| 5555 | Broker | REQ (clientes) |
| 5556 | Broker | REP (servidores) |
| 5557 | Proxy | XSUB (publishers) |
| 5558 | Proxy | XPUB (subscribers) |
| 5559 | Servidor | PUB (mensagens) |
| 5560 | ReferÃªncia | Rank, List, Heartbeat |
| 5561 | Servidor | SincronizaÃ§Ã£o (clock, election) |
| **5562** | **Servidor** | **ReplicaÃ§Ã£o de dados** |

## ğŸš€ Como Executar

### Build e Iniciar

```bash
docker-compose build
docker-compose up -d
```

### Ver Logs

```bash
# Todos os serviÃ§os
docker-compose logs -f

# Apenas servidores
docker-compose logs -f servidor

# Apenas broker
docker-compose logs -f broker
```

### Usar Cliente Interativo

```bash
docker attach cliente

# Menu:
# [1] - Login
# [2] - Listar usuÃ¡rios
# [3] - Cadastrar canal
# [4] - Listar canais
# [5] - Publicar em canal
# [6] - Mensagem privada
# [0] - Sair
```

### Limpar Dados

```bash
# Parar e remover volumes
docker-compose down -v

# Rebuild
docker-compose build
docker-compose up
```

## ğŸ“ˆ Logs de ReplicaÃ§Ã£o

### Servidor que Recebe Cliente

```
[S] - Login do alice feito!
[S] âœ… 1 usuÃ¡rios salvos em usuarios.json
```

### Servidores que Recebem ReplicaÃ§Ã£o

```
[S] ReplicaÃ§Ã£o: usuÃ¡rio 'alice' adicionado
[S] âœ… 2 usuÃ¡rios salvos em usuarios.json
```

### VerificaÃ§Ã£o de ConsistÃªncia

```
[S] Listando 3 usuÃ¡rios cadastrados:
    Usuario 0: alice | timestamp: 1699547123.456
    Usuario 1: bob | timestamp: 1699547134.789
    Usuario 2: charlie | timestamp: 1699547145.012
```

Todos os servidores mostram a mesma lista! âœ…

## ğŸ¯ ConclusÃ£o

### Vantagens da SoluÃ§Ã£o

1. âœ… **ConsistÃªncia eventual** garantida
2. âœ… **Alta disponibilidade** (AP no CAP)
3. âœ… **ReplicaÃ§Ã£o assÃ­ncrona** nÃ£o afeta latÃªncia
4. âœ… **Descoberta dinÃ¢mica** de servidores
5. âœ… **Dupla garantia**: Volume + ReplicaÃ§Ã£o
6. âœ… **Sem perda de dados** (assumindo â‰¥1 servidor ativo)

### LimitaÃ§Ãµes Conhecidas

1. âš ï¸ **InconsistÃªncia temporÃ¡ria** (< 1s)
2. âš ï¸ **Conflitos nÃ£o detectados** (usa first-write-wins)
3. âš ï¸ **Sem transaÃ§Ãµes distribuÃ­das**
4. âš ï¸ **Requer pelo menos 1 servidor ativo**

### Melhorias Futuras

- [ ] Quorum de escritas (majority)
- [ ] DetecÃ§Ã£o e resoluÃ§Ã£o de conflitos
- [ ] ReplicaÃ§Ã£o sÃ­ncrona opcional
- [ ] CompactaÃ§Ã£o de logs
- [ ] Snapshot periÃ³dico

## ğŸ“š Tecnologias Utilizadas

- **Python 3.13**
- **ZeroMQ (pyzmq)** - ComunicaÃ§Ã£o
- **MessagePack** - SerializaÃ§Ã£o
- **Docker & Docker Compose** - OrquestraÃ§Ã£o
- **JSON** - PersistÃªncia

## ğŸ‘¥ Componentes

- **3 Servidores** (rÃ©plicas)
- **1 Broker** (load balancer)
- **1 Proxy** (pub/sub)
- **1 Servidor de ReferÃªncia** (coordenaÃ§Ã£o)
- **1 Publisher** (intermediÃ¡rio)
- **1 Subscriber** (consumidor)
- **1 Cliente** (interativo)
- **2 Clientes AutomÃ¡ticos** (bots)

---

ğŸ‰ **Projeto completo com replicaÃ§Ã£o e consistÃªncia eventual!**

